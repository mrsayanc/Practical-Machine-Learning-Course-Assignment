---
title: "Practical_Machine_Learning_Assignment"
author: "Sayan Chakraborty"
date: "January 12, 2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Data import

1. Set file names and import data

```{r import}
# IMPORT THE DATA 

train_file <- "C:/Users/user/Downloads/ML/Practical Machine Learning/pml-training.csv"
test_file  <- "C:/Users/user/Downloads/ML/Practical Machine Learning/pml-testing.csv"

development <- read.csv(train_file, row.names=1)
validation  <- read.csv(test_file, row.names=1)

# CHECK IMPORT STATUS AND ROWS AND COLUMNS
dim(development)
dim(validation)
```

## Intialise

1. Importing the libraries and setting the seed.

2. Splitting the training data into train and test sample (70% - 30%)

3. Create a cross validation framework

```{r intialise}
# 1. Import libraries
library(AppliedPredictiveModeling)
library(caret)
library(pgmm)
library(rpart)
library(gbm)
library(lubridate)
library(forecast)
library(e1071)
library(lattice)
library(rpart)
library(rattle)

# set seed
set.seed(999)

# 2. SPLIT INTO DEVELOPMENT AND VALIDATION
train_split <- createDataPartition(y=development$classe, p=0.7, list=FALSE)
train_data <- development[ train_split,]
test_data  <- development[-train_split,]

dim(train_data)
dim(test_data)

# 3. Create coss validation Framework
cross_validation <- trainControl(method='cv', number = 3)

```

## Variable Selection

Dropping all variables that are

1. Irrelevant

2. Has no variance

3. Has a high missing rate (taking > 80% as rejection criteria)

```{r varselect}
names(train_data)

# 1. Remove variables that are not intuitive
# 4 dropped - "user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp"
col_list <- c(1:4)
train_data_dropped <- train_data[,-col_list]

# 2. Remove variables with no variance
concentration <- nearZeroVar(train_data_dropped)
length(concentration)

train_data_trimmed <- train_data_dropped[,-concentration]
dim(train_data_trimmed)

# 3. Check for columns with high missing rates (more than 80%)
NA_cols <- sapply(train_data_trimmed, function(x) mean(is.na(x))) > 0.80
print(sum(NA_cols))

train_data_final <- train_data_trimmed[, NA_cols==FALSE]
dim(train_data_final)

```

## Decision Tree

Run a CART model on the train and then validate the performance on the test

```{r cart}
CART <- rpart(classe ~ ., data = train_data_final, method="class")
fancyRpartPlot(CART)

# Now test the accuracy
pred_CART <- predict(CART, newdata=test_data, type="class")
perf_CART <- confusionMatrix(pred_CART, test_data$classe)
perf_CART

```

## Gradient Boosting

Run a GBM model on the train with cross validation and then validate the performance on the test

```{r gbm}
# First train the GBM
GBM <- train(classe ~ ., data = train_data_final, method = "gbm",trControl=cross_validation, verbose = FALSE)

# Now test the accuracy
pred_GBM <- predict(GBM, newdata=test_data)
perf_GBM <- confusionMatrix(pred_GBM, test_data$classe)
perf_GBM
```

## Random Forest

Run a Random Forest model on the train with cross validation and then validate the performance on the test

```{r rf}
# First train the random forest
RF  <- train(classe ~ ., data = train_data_final, method = "rf",trControl=cross_validation, verbose = FALSE)

# Now test the accuracy
pred_RF <- predict(RF, newdata=test_data)
perf_RF <- confusionMatrix(pred_RF, test_data$classe)
perf_RF
```
## Prediction

Since the Random Forest works best use it to predict

```{r predict}
pred_RF_final <- predict(RF, newdata=validation)
pred_RF_final
```
```


